{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52650ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Link</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Job_posted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst Junior (H/F)</td>\n",
       "      <td>https://fr.linkedin.com/jobs/view/data-analyst...</td>\n",
       "      <td>Akademija Oxford</td>\n",
       "      <td>Boulogne-Billancourt, Île-de-France, France</td>\n",
       "      <td>3 weeks ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst en alternance</td>\n",
       "      <td>https://fr.linkedin.com/jobs/view/data-analyst...</td>\n",
       "      <td>Akademija Oxford</td>\n",
       "      <td>Ille-et-Vilaine, Brittany, France</td>\n",
       "      <td>4 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst Junior (H/F)</td>\n",
       "      <td>https://fr.linkedin.com/jobs/view/data-analyst...</td>\n",
       "      <td>Akademija Oxford</td>\n",
       "      <td>Boulogne-Billancourt, Île-de-France, France</td>\n",
       "      <td>2 weeks ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data analyst</td>\n",
       "      <td>https://fr.linkedin.com/jobs/view/data-analyst...</td>\n",
       "      <td>Akademija Oxford</td>\n",
       "      <td>Boulogne-Billancourt, Île-de-France, France</td>\n",
       "      <td>2 weeks ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>https://fr.linkedin.com/jobs/view/data-analyst...</td>\n",
       "      <td>Akademija Oxford</td>\n",
       "      <td>Ille-et-Vilaine, Brittany, France</td>\n",
       "      <td>2 weeks ago</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Title  \\\n",
       "0   Data Analyst Junior (H/F)   \n",
       "1  Data Analyst en alternance   \n",
       "2   Data Analyst Junior (H/F)   \n",
       "3                Data analyst   \n",
       "4                Data Analyst   \n",
       "\n",
       "                                                Link           Company  \\\n",
       "0  https://fr.linkedin.com/jobs/view/data-analyst...  Akademija Oxford   \n",
       "1  https://fr.linkedin.com/jobs/view/data-analyst...  Akademija Oxford   \n",
       "2  https://fr.linkedin.com/jobs/view/data-analyst...  Akademija Oxford   \n",
       "3  https://fr.linkedin.com/jobs/view/data-analyst...  Akademija Oxford   \n",
       "4  https://fr.linkedin.com/jobs/view/data-analyst...  Akademija Oxford   \n",
       "\n",
       "                                      Location   Job_posted  \n",
       "0  Boulogne-Billancourt, Île-de-France, France  3 weeks ago  \n",
       "1            Ille-et-Vilaine, Brittany, France   4 days ago  \n",
       "2  Boulogne-Billancourt, Île-de-France, France  2 weeks ago  \n",
       "3  Boulogne-Billancourt, Île-de-France, France  2 weeks ago  \n",
       "4            Ille-et-Vilaine, Brittany, France  2 weeks ago  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "## Récupérer le code source\n",
    "def scraper(page):\n",
    "    ## Le lien de linkedin avec mot clé recherché \"DATA\"\n",
    "    url='https://www.linkedin.com/jobs/search?keywords=Data&location=France&geoId=105015875&trk=public_jobs_jobs-search-bar_search-submit&original_referer=https%3A%2F%2Fwww.linkedin.com%2Fjobs%2Fsearch%3Fkeywords%3D%26location%3DFrance%26geoId%3D105015875%26f_TPR%3Dr2592000%26position%3D1%26pageNum%3D0&position=1&pageNum=0'    \n",
    "    \n",
    "    ## User-Agent permet de spécifier le navigateur qui va accèder a la page web. \n",
    "    ## Il envoie ces chaines de caractère 'Mozilla/5.0 (Windows NT 10.0...' qui ont pour objectif d'optimiser la page web en fonction de notre matériel et de vos logiciels.\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:123.0) Gecko/20100101 Firefox/123.0'}\n",
    "    \n",
    "    ## requests permet de lire la page web\n",
    "    r = requests.get(url,headers) \n",
    "    \n",
    "    ## B4s permet de récupérer le contenu du code source de la page web, c'est-à-dire extraire des données de fichiers HTML et XML. \n",
    "    soup = BeautifulSoup(r.content, 'lxml')\n",
    "    \n",
    "    return soup\n",
    "\n",
    "## Extraire les informations pertinentes sur les offres d'emploi\n",
    "def extract(soup):\n",
    "    ## On recherche dans le code source toutes les balises <div> ayant la class 'base-card.....'. cette balise + class contient les informations sur les offres d'emploi LinkendIn\n",
    "    jobs = soup.find_all('div', class_='base-card relative w-full hover:no-underline focus:no-underline base-card--link base-search-card base-search-card--link job-search-card' )\n",
    "    \n",
    "    for item in jobs:\n",
    "        ## Extraire le titre de l'offre d'emploi à partir du code HTML en trouvant le premier élément <a> à l'intérieur de la balise <div> ayant la class 'base-card.....'. Puis supprime tout espace inutile \"strip()\"\n",
    "        title = item.find('a').text.strip()\n",
    "        \n",
    "        ## Même lecture que le commentaire de title\n",
    "        company = item.find('h4').text.strip()\n",
    "        \n",
    "        ## Accéder à l'attribut 'href' de l'élément <a>. L'attribut 'href' est utilisé pour spécifier l'URL de destination du lien.\n",
    "        link = item.a['href']\n",
    "        \n",
    "        ## Même lecture que le commentaire de title\n",
    "        location = item.find('span',class_='job-search-card__location').text.strip()\n",
    "        \n",
    "        ## Même lecture que le commentaire de title\n",
    "        Date_posted = item.find('time', class_='job-search-card__listdate')\n",
    "        \n",
    "        ## Cette instruction vérifie si l'élément Date_posted existe dans la structure HTML.\n",
    "        ## Si Date_posted existe, donc \n",
    "        if Date_posted is not None:\n",
    "            ## Extrait le texte de Date_posted\n",
    "            time = Date_posted.text.strip()\n",
    "            ## Ensuite, stocker les résultats de l'offre d'emploi dans le dictionnaire\n",
    "            job  = {\n",
    "                'Title' : title,\n",
    "                'Link' : link,\n",
    "                'Company': company,\n",
    "                'Location': location,\n",
    "                'Job_posted':time }\n",
    "            \n",
    "            ## Une fois que le dictionnaire job est créé, il est ajouté à la liste \"joblist\"\n",
    "            joblist.append(job)\n",
    "    return\n",
    "            \n",
    "      \n",
    "## créer une liste vide         \n",
    "joblist =[]       \n",
    "\n",
    "##\n",
    "for i in range (0,300, 1):\n",
    "    ## Exécution de la fonction scraper qui va Scaper les page de 0 a 299 (inclus).\n",
    "    s = scraper(i)\n",
    "    ## Ensuite, la fonction extract() est appelée avec l'objet soup retourné par scraper(). \n",
    "    ## Cette fonction extrait les détails des offres d'emploi de la page et les ajoute à la liste joblist.\n",
    "    c = extract(s)\n",
    "\n",
    "df = pd.DataFrame(joblist)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6299502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Linkedin_job_list.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
